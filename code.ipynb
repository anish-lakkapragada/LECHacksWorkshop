{"cells":[{"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kentonishi/nlp-preprocessing-demo/blob/master/solution.ipynb)"]},{"cell_type":"markdown","metadata":{},"source":["# yay, let's demo what we learned today!"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting nltk\n","  Using cached https://files.pythonhosted.org/packages/aa/b8/09ac15436591cefc0adc882798d5cf629f13addae0495b20b682219e3afe/nltk-3.6.5-py3-none-any.whl\n","Requirement already satisfied: joblib in /Users/anish/Library/Python/3.8/lib/python/site-packages (from nltk) (1.0.1)\n","Requirement already satisfied: tqdm in /Users/anish/Library/Python/3.8/lib/python/site-packages (from nltk) (4.60.0)\n","Collecting regex>=2021.8.3 (from nltk)\n","  Using cached https://files.pythonhosted.org/packages/b6/53/fea3a3ffaa05b7787bfd359ddeb0f42e5da925da34c0bef158c566248e37/regex-2021.11.10-cp38-cp38-macosx_10_9_x86_64.whl\n","Requirement already satisfied: click in /Users/anish/Library/Python/3.8/lib/python/site-packages (from nltk) (7.1.2)\n","Installing collected packages: regex, nltk\n","  Found existing installation: regex 2021.4.4\n","    Uninstalling regex-2021.4.4:\n","      Successfully uninstalled regex-2021.4.4\n","Successfully installed nltk-3.6.5 regex-2021.11.10\n","\u001b[33mWARNING: You are using pip version 19.2.3, however version 21.3.1 is available.\n","You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"]}],"source":["# prerequisites\n","!pip3.8 install nltk # library for natural language processing"]},{"cell_type":"markdown","metadata":{},"source":["# import the libraries we'll be using (just code other ppl wrote that we can use too)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /Users/anish/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# import libraries we need\n","import re  # regular expressions for splitting strings\n","import nltk  # natural language toolkit\n","from nltk import word_tokenize  # tokenize sentences\n","from nltk.corpus import stopwords  # list of stopwords\n","from nltk.stem.porter import PorterStemmer  # stemming algorithm\n","\n","# just run this \n","nltk.download('punkt')"]},{"cell_type":"markdown","metadata":{},"source":["# first step: preprocessing!"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# the string we'll be working with\n","TEXT = \"LEC Hacks is an incredibly good hackathon and we are definitely not sponsored to say this.\""]},{"cell_type":"markdown","metadata":{},"source":["# split the words into a list"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['LEC', 'Hacks', 'is', 'an', 'incredibly', 'good', 'hackathon', 'and', 'we', 'are', 'definitely', 'not', 'sponsored', 'to', 'say', 'this', '.']\n","['LEC', 'Hacks', 'is', 'an', 'incredibly', 'good', 'hackathon', 'and', 'we', 'are', 'definitely', 'not', 'sponsored', 'to', 'say', 'this']\n"]}],"source":["tokens = word_tokenize(TEXT) \n","print(tokens)\n","\n","# we also don't need to have punctuation in our text\n","words = [word for word in tokens if word.isalpha()]  \n","print(words) # removes the punctuation "]},{"cell_type":"markdown","metadata":{},"source":["# Remove stop words (unimportant words with no meaning)."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /Users/anish/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /Users/anish/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["nltk.download(\"punkt\") \n","nltk.download(\"stopwords\")\n","\n","stop_words = (stopwords.words(\"english\"))  # read stopwords\n","print(stop_words)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['LEC', 'Hacks', 'incredibly', 'good', 'hackathon', 'definitely', 'sponsored', 'say']\n"]}],"source":["# let's remove these words \n","words = [word for word in words if not word in stop_words]  # only select non-stopwords\n","print(words)"]},{"cell_type":"markdown","metadata":{},"source":["# we can also use stemming, to convert words to their base form: \n","\n","## for example, \"eating\" => \"eat\". Only applied for words without a stem"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["lec\n","eat\n"]}],"source":["porter = PorterStemmer() \n","\n","NO_STEM_WORD = \"LEC\" \n","STEM_WORD = \"EATING\"\n","\n","print(porter.stem(NO_STEM_WORD)) # no stem, just changed capitalization\n","print(porter.stem(STEM_WORD)) # stem"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['lec', 'hack', 'incred', 'good', 'hackathon', 'definit', 'sponsor', 'say']\n"]}],"source":["words = [porter.stem(word) for word in words]  # stem all words\n","print(words)"]},{"cell_type":"markdown","metadata":{},"source":["# part 2 : exploring word embeddings!"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gensim in /Users/anish/miniconda3/lib/python3.8/site-packages (4.1.2)\n","Requirement already satisfied: scipy>=0.18.1 in /Users/anish/miniconda3/lib/python3.8/site-packages (from gensim) (1.6.3)\n","Requirement already satisfied: numpy>=1.17.0 in /Users/anish/miniconda3/lib/python3.8/site-packages (from gensim) (1.20.3)\n","Requirement already satisfied: smart-open>=1.8.1 in /Users/anish/miniconda3/lib/python3.8/site-packages (from gensim) (5.2.1)\n","Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (4.1.2)\n","Requirement already satisfied: scipy>=0.18.1 in /Users/anish/Library/Python/3.8/lib/python/site-packages (from gensim) (1.6.2)\n","Requirement already satisfied: numpy>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gensim) (1.18.5)\n","Requirement already satisfied: smart-open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gensim) (5.2.1)\n"]}],"source":["# first install what we'll need \n","!pip install gensim \n","!pip3.8 install gensim "]},{"cell_type":"markdown","metadata":{},"source":["# Load the Stanford GLoVE Vectors (100 sized)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["import gensim\n","glove_vectors = gensim.downloader.load('glove-wiki-gigaword-100')"]},{"cell_type":"markdown","metadata":{},"source":["# try it out "]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"data":{"text/plain":["(100,)"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["WORD = 'word'\n","glove_vectors[WORD].shape"]},{"cell_type":"markdown","metadata":{},"source":["# check most similar words (basically prove this works.)"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"data":{"text/plain":["[('dingy', 0.7442813515663147),\n"," ('windowless', 0.6911330223083496),\n"," ('claustrophobic', 0.683182954788208),\n"," ('musty', 0.6824417114257812),\n"," ('grimy', 0.6645137071609497),\n"," ('filthy', 0.6520333290100098),\n"," ('cramped', 0.6409538388252258),\n"," ('dreary', 0.613743782043457),\n"," ('subterranean', 0.6083458662033081),\n"," ('drafty', 0.6067347526550293)]"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["glove_vectors.most_similar(\"dank\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"},"kernelspec":{"display_name":"Python 3.8.3 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"orig_nbformat":2},"nbformat":4,"nbformat_minor":2}
